{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":34674,"sourceType":"datasetVersion","datasetId":26069},{"sourceId":12523177,"sourceType":"datasetVersion","datasetId":7905022},{"sourceId":12523350,"sourceType":"datasetVersion","datasetId":7905141},{"sourceId":12550321,"sourceType":"datasetVersion","datasetId":7924084}],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\n#for dirname, _, filenames in os.walk('/kaggle/input'):\n#    for filename in filenames:\n#        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"## Metadata from manifest + compact_reads.\nimport pandas as pd\n\nmetadata_df = pq.merge(\n    compact_reads[['name', 'ICH-majority']],\n    on='name',\n    how='left',\n    validate='many_to_one',\n    indicator=True\n)\nmetadata_df.to_parquet(\"cq500ct_metadata.parquet\", index=False)","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"## Split the B1 category only into train and validation sets\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import StratifiedGroupKFold # Cross-Validation phase\n\nmeta = pd.read_parquet(\"/kaggle/input/metadata/cq500ct_metadata.parquet\") # metadata\nmeta = meta.drop(\"_merge\", axis=1)\ncat = pd.read_csv(\"/kaggle/input/b1-cat-reads/b1_cat_reads.csv\") # categories\n\ndf = meta.merge(\n    cat[[\"name\", \"Category\"]],\n    on=\"name\",\n    how=\"left\",\n    indicator=True\n)","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"## Create B1-only metadata\nb1_df = df[df[\"Category\"] == \"B1\"].copy()\nb1_df.to_parquet(\"b1_metadata.parquet\", index=False)","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"## Split the data for training.\nimport pandas as pd\nfrom sklearn.model_selection import StratifiedGroupKFold\n\nmeta_df = pd.read_parquet(\"/kaggle/input/b1-cat-meta/b1_metadata.parquet\") # load files\nsgkf = StratifiedGroupKFold(n_splits=5, shuffle=True, random_state=42)\n\nfor fold, (train_idx, val_idx) in enumerate(\n    sgkf.split(X=meta_df[[\"name\"]],\n    y=meta_df[\"ICH-majority\"], groups=meta_df[\"name\"])\n):\n    if fold == 0:\n        train_patients = meta_df.iloc[train_idx][\"name\"].tolist()\n        val_patients = meta_df.iloc[val_idx][\"name\"].tolist()\n        break\nprint(f\"train = {len(train_patients)}, val = {len(val_patients)}\")\n\ntrain_meta = meta_df[meta_df[\"name\"].isin(train_patients)]\nval_meta = meta_df[meta_df[\"name\"].isin(val_patients)]\ntrain_meta[[\"name\"]].to_parquet(\"train_patients.parquet\", index=False)\nval_meta[[\"name\"]].to_parquet(\"val_patients.parquet\", index=False)","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"## Check leakage between train and validation indecies\nassert set(meta_df.iloc[train_idx].name).isdisjoint(meta_df.iloc[val_idx].name), \"Leakage\"","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# DataLoader Class for 2.5D CNN\n# Imports\nfrom pathlib import Path\nfrom typing import List, Sequence, Tuple, Dict, Optional\n\nimport os\nimport numpy as np\nimport pandas as pd\nimport torch\nfrom torch.utils.data import Dataset\nimport cv2\nimport pydicom\nimport pydicom.dataset\nfrom pydicom.pixel_data_handlers.util import _apply_modality_lut\n\n# Helper\n## DICOM to Tensor\ndef dcm_to_tensor(\n    path: str,\n    windows: list[tuple[int, int]] = None,\n    out_size: tuple[int, int] = None,\n    dtype: torch.dtype = torch.float32,\n) -> torch.Tensor:\n    \"\"\"\n    Helper: convert one DICOM slice into a torch.Tensor [C x H x W]\n    ds\t\t\t= single CQ500 DICOM slice.\n    windows\t\t= CT window to apply. Each tuple becomes one output channel.\n                    Default = Brain, Subdural, Bone.\n    out_size\t= (H, W) to resize the slice.\n    dtype\t\t= final tensor percision (float32 recommended).\n\n    Returns\t\t= torch.tensor (shape = C x H x W with normalized values)\n    \"\"\"\n    ## Handle None args\n    if windows is None:\n        windows = [(40, 80), (80, 200), (600, 2800)]\n    if out_size is None:\n        out_size = (256, 256)\n\n    ds: pydicom.dataset.FileDataset = pydicom.dcmread(str(path))\n\n    ## Raw values > Hounsfield units (HU)\n    hu: np.ndarray = _apply_modality_lut(ds.pixel_array, ds).astype(np.int16)\n    if out_size is not None and hu.shape != out_size:\n        hu = cv2.resize(hu, out_size[::-1], interpolation=cv2.INTER_LINEAR)\n\n    ## Window / Level > 0-1 float per channel\n    ## 3 channels to feed into model\n    chans: list[np.ndarray] = []\n    for level, width in windows:\n        level: int\n        width: int\n        lower: int = level - (width // 2)\n        upper: int = level + (width // 2)\n        img_clipped: np.ndarray = np.clip(hu, lower, upper)\n        img_norm: np.ndarray = (img_clipped - lower) / float(width)  # 0 - 1\n        chans.append(img_norm.astype(np.float32))\n\n    ## Stack and convert to tensor\n    arr: np.ndarray = np.stack(chans, axis=0)  # C x H x W\n    tensor: torch.Tensor = torch.from_numpy(arr).type(dtype)\n\n    return tensor\n\n\n# Class\nclass CQ500DataLoader25D(Dataset):\n    \"\"\"\n    Iterable dataset that yields (x, y) pairs.\n    x = (3, H, W) windows for slice i, with slice context \\\n    [i-1, i, i+1].\n    y = series-level ICH label.\n    Optionally caches complete series volumes in RAM.\n    \"\"\"\n    def __init__(\n        self,\n        metadata_path: str | Path,\n        indices: Optional[Sequence[int]] = None,\n        transform = None,\n        cache: bool = False,\n        replicate_edge: bool = True,\n    ) -> None:\n        super().__init__()\n        self.df = pd.read_parquet(metadata_path)\n\n        ## Restric to desired subset of patients (int idx list)\n        grouped = self.df.groupby(\"name\", sort = False)\n        self.patients: List[Tuple[str, pd.DataFrame]] = list(grouped)\n        if indices is not None:\n            if isinstance(indices[0], int):\n                self.patients = [self.patients[i] for i in indices]\n            elif isinstance(indices[0], str):\n                name_to_idx = {name: (name, pdf) for name, pdf in self.patients}\n                self.patients = [name_to_idx[name] for name in indices if name in name_to_idx]\n            else:\n                raise ValueError(\n                    \"The argument for indices must be a list of int or str (patient names)\"\n                )\n\n        ## Flatten into sample idx list (patient_idx, slice_idx)\n        self.sample_index: List[Tuple[int, int]] = []\n        for p_idx, (_, pdf) in enumerate(self.patients):\n            pdf_sorted = pdf.sort_values(\"instance_num\")\n            n_slices = len(pdf_sorted)\n            for s_idx in range(n_slices):\n                self.sample_index.append((p_idx, s_idx))\n\n        self.transform = transform\n        self.cache_enabled = cache\n        self.replicate_edge = replicate_edge\n        self._series_cache: Dict[str, torch.Tensor] = {}\n\n        ## Cache if caching is set to True\n        if self.cache_enabled:\n            self._populate_cache()\n\n    def __len__(self) -> int:\n        return len(self.sample_index)\n\n    def _populate_cache(self) -> None:\n        \"\"\"\n        Preload all series volumes into RAM (lazy-converted to torch.Tensor).\n        \"\"\"\n        print(\"[CQ500DataLoader25D] Caching series data …\")\n        for patient_name, pdf in self.patients:\n            pdf_sorted = pdf.sort_values(\"instance_num\")\n            slices = [dcm_to_tensor(path = p) for p in pdf_sorted[\"path\"].tolist()]\n            vol_np = np.stack(slices, axis=0)  # (S, 3, H, W)\n            self._series_cache[patient_name] = torch.from_numpy(vol_np)\n        print(f\"→ Cached {len(self._series_cache)} series.\")\n\n    ## Public API\n    def enable_cache(self) -> None:\n        \"\"\"Enable caching automatically.\"\"\"\n        if not self.cache_enabled:\n            self.cache_enabled = True\n            self._populate_cache()\n    def disable_cache(self) -> None:\n        \"\"\"Disable caching automatically.\"\"\"\n        if self.cache_enabled:\n            self.cache_enabled = False\n            self._series_cache.clear()\n\n    def __getitem__(\n        self, idx: int\n    ) -> Tuple[torch.Tensor, torch.Tensor]:\n        patient_idx, slice_idx = self.sample_index[idx]\n        patient_name, pdf = self.patients[patient_idx]\n        label = torch.tensor(pdf[\"ICH-majority\"].iloc[0], dtype=torch.float32)\n\n        # Load the volume form cache or disk on-the-fly\n        if self.cache_enabled and patient_name in self._series_cache:\n            volume = self._series_cache[patient_name]   # (S, 3, H, W) torch.Tensor\n        else:\n            pdf_sorted = pdf.sort_values(\"instance_num\")\n            paths = pdf_sorted[\"path\"].tolist()\n            slices = [dcm_to_tensor(path = p) for p in paths]\n            volume = torch.from_numpy(np.stack(slices, axis = 0))   # (S, 3, H, W)\n\n        n_slices = volume.shape[0]\n\n        # Determine neighboring indices with optional edge replication\n        def safe_index(i: int) -> int:\n            if self.replicate_edge:\n                return max(0, min(i, n_slices - 1))\n            return i\n        prev_idx = safe_index(slice_idx - 1)\n        next_idx = safe_index(slice_idx + 1)\n\n        x_stack = torch.stack(\n            [volume[prev_idx], volume[slice_idx], volume[next_idx]], dim = 0\n        )   # (3, 3, H, W)\n        ## Merge HU channel and slice context dims (slice_ctx, hu_ch, H, W).\\\n        ## The convention is to keep the slice context and drop the HU channel.\\\n        ## Because we windowed into HU spaces, each slice context acts as one channel.\\\n        x = x_stack.mean(dim = 1)   # (3, H, W) - dim=1 for HU channel\n        # x = x_stack.permute(1, 0, 2, 3).reshape(-1, x_stack.shape[2], x_stack.shape[3]) # (9, H, W)\n\n        if self.transform:\n            x = self.transform(x)\n\n        return x, label\n\n    # Preprocess to .pt files\n    def preprocess(\n        self, output_dir: str,\n        chunk_size: int = 100,\n        max_ram_gb: int = 12\n    ) -> None:\n        \"\"\"\n        Preprocess DICOMs: convert to tensors and save to disk \\\n        in small chunks to avoid exceeding RAM.\n        Args:\n            output_dir: Directory to save tensors (will be created if not exists).\n            chunk_size: Number of slices to process and save at once.\n            max_ram_gb: Maximum RAM (in GB) to use for holding tensors in memory at once.\n        \"\"\"\n        os.makedirs(output_dir, exist_ok=True)\n        tensor_buffer = []\n        meta_buffer = []\n        buffer_bytes = 0\n        max_bytes = max_ram_gb * 1024 ** 3\n        chunk_idx = 0\n        print(f\"[Preprocess] Saving tensors to {output_dir} \\\n            in chunks of {chunk_size} slices, max RAM {max_ram_gb} GB...\")\n        for patient_name, pdf in self.patients:\n            pdf_sorted = pdf.sort_values(\"instance_num\")\n            for _, row in pdf_sorted.iterrows():\n                dcm_path = row[\"path\"]\n                instance_num = row[\"instance_num\"]\n                tensor = dcm_to_tensor(dcm_path)\n                tensor_buffer.append(tensor)\n                meta_buffer.append((patient_name, instance_num, dcm_path))\n                buffer_bytes += tensor.element_size() * tensor.nelement()\n                # Save chunk if buffer is large enough\n                if len(tensor_buffer) >= chunk_size or buffer_bytes >= max_bytes:\n                    chunk_file = os.path.join(output_dir, f\"chunk_{chunk_idx:05d}.pt\")\n                    meta_file = os.path.join(output_dir, f\"chunk_{chunk_idx:05d}_meta.csv\")\n                    torch.save(torch.stack(tensor_buffer), chunk_file)\n                    pd.DataFrame(\n                        meta_buffer, columns=[\"patient_name\", \"instance_num\", \"dcm_path\"]\n                    ).to_csv(meta_file, index=False)\n                    print(f\"  Saved {len(tensor_buffer)} slices to {chunk_file}\")\n                    tensor_buffer.clear()\n                    meta_buffer.clear()\n                    buffer_bytes = 0\n                    chunk_idx += 1\n        # Save any remaining tensors\n        if tensor_buffer:\n            chunk_file = os.path.join(output_dir, f\"chunk_{chunk_idx:05d}.pt\")\n            meta_file = os.path.join(output_dir, f\"chunk_{chunk_idx:05d}_meta.csv\")\n            torch.save(torch.stack(tensor_buffer), chunk_file)\n            pd.DataFrame(\n                meta_buffer, columns=[\"patient_name\", \"instance_num\", \"dcm_path\"]\n            ).to_csv(meta_file, index=False)\n            print(f\"  Saved {len(tensor_buffer)} slices to {chunk_file}\")\n        print(\"[Preprocess] Done.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-23T08:30:31.264850Z","iopub.execute_input":"2025-07-23T08:30:31.265143Z","iopub.status.idle":"2025-07-23T08:30:31.288830Z","shell.execute_reply.started":"2025-07-23T08:30:31.265122Z","shell.execute_reply":"2025-07-23T08:30:31.288011Z"}},"outputs":[],"execution_count":22},{"cell_type":"code","source":"# Model Class for 2.5 CNN (WIP)\n\"\"\"\nModel pipeline\n\"\"\"\n# 2p5d_cnn_train.py\nimport torch.nn as nn\nimport torch, torchvision as tv\nfrom torch.utils.data import DataLoader\nfrom torch.amp import autocast, GradScaler\nfrom torchmetrics.classification import (\n    BinaryAUROC, BinaryAveragePrecision,\n    BinaryRecall, BinarySpecificity\n)\nfrom tqdm.notebook import tqdm\n\n\n# ---------- 1. Model ----------------------------------------------------------\ndef _replace_first_conv(m: nn.Module, in_ch: int) -> None:\n    \"\"\"Replace the first conv to accept `in_ch` channels; keeps pretrained weights.\"\"\"\n    old = m.conv1\n    new = nn.Conv2d(in_ch, old.out_channels,\n                    kernel_size=old.kernel_size,\n                    stride=old.stride,\n                    padding=old.padding,\n                    bias=old.bias is not None)\n    # repeat / average weights to new conv (simple heuristic)\n    with torch.no_grad():\n        repeat = in_ch // old.in_channels\n        new.weight.copy_(old.weight.repeat(1, repeat, 1, 1) / repeat)\n    m.conv1 = new\n\n\n_BACKBONES = {\n    \"resnet18\": lambda ic: tv.models.resnet18(weights=\"IMAGENET1K_V1\"),\n    \"resnet34\": lambda ic: tv.models.resnet34(weights=\"IMAGENET1K_V1\"),\n    \"densenet121\": lambda ic: tv.models.densenet121(weights=\"IMAGENET1K_V1\"),\n    # add more here...\n}\n\n\nclass TwoPointFiveD(nn.Module):\n    \"\"\" Backbone + Linear head for binary ICH classification \"\"\"\n    def __init__(self, backbone_name: str = \"resnet18\",\n                 in_channels: int = 3):            # 3 slices (HU channel is reduced)\n        super().__init__()\n        backbone = _BACKBONES[backbone_name](in_channels)\n        _replace_first_conv(backbone, in_channels)\n\n        if hasattr(backbone, \"fc\"):               # ResNet-style\n            feat_dim = backbone.fc.in_features\n            backbone.fc = nn.Identity()\n        elif hasattr(backbone, \"classifier\"):     # DenseNet-style\n            feat_dim = backbone.classifier.in_features\n            backbone.classifier = nn.Identity()\n        else:\n            raise ValueError(\"Add support for this backbone\")\n\n        self.backbone = backbone\n        self.classifier = nn.Linear(feat_dim, 1)  # logits\n\n    def forward(self, x):\n        \"\"\" Set the model and return Classifier \"\"\"\n        x = self.backbone(x)\n        return self.classifier(x).squeeze(1)      # (N,) logits\n\n\n# ---------- 2. Metrics helpers -----------------------------------------------\ndef make_metric_dict(device):\n    \"\"\" Make metrics dictionary \"\"\"\n    return {\n        \"auroc\": BinaryAUROC().to(device),\n        \"prauc\": BinaryAveragePrecision().to(device),\n        \"sens\":  BinaryRecall(threshold=0.5).to(device),        # sensitivity\n        \"spec\":  BinarySpecificity(threshold=0.5).to(device)    # specificity\n    }\n\n\ndef update_metrics(metrics, preds, targets):\n    \"\"\" Update the metrics \"\"\"\n    for m in metrics.values():\n        m.update(preds, targets.int())\n\n\ndef compute_and_reset(metrics):\n    \"\"\" Compute Metrics \"\"\"\n    out = {k: float(v.compute()) for k, v in metrics.items()}\n    for v in metrics.values():\n        v.reset()\n    return out\n\n\n# ---------- 3. Train / Val loops ---------------------------------------------\n@torch.no_grad()\ndef validate(model, loader, loss_fn, device, metrics):\n    \"\"\" Validate \"\"\"\n    model.eval()\n    loop = tqdm(loader, desc=\"val\", leave=False)\n    total_loss = 0.0\n    for x, y in loop:\n        x, y = x.to(device, non_blocking=True), y.float().to(device, non_blocking=True)\n        logits = model(x)\n        loss = loss_fn(logits, y)\n        total_loss += loss.item() * y.size(0)\n\n        probs = torch.sigmoid(logits)\n        update_metrics(metrics, probs, y)\n    stats = compute_and_reset(metrics)\n    stats[\"loss\"] = total_loss / len(loader.dataset)\n    return stats\n\n\ndef train_one_epoch(model, loader, optimizer, scaler, loss_fn,\n                    device, metrics, epoch):\n    \"\"\" Train one Epoch \"\"\"\n    model.train()\n    loop = tqdm(loader, desc=f\"train {epoch}\")\n    for x, y in loop:\n        x, y = x.to(device, non_blocking=True), y.float().to(device, non_blocking=True)\n\n        optimizer.zero_grad(set_to_none=True)\n        with autocast(device_type=device.type):\n            logits = model(x)\n            loss = loss_fn(logits, y)\n\n        scaler.scale(loss).backward()\n        scaler.step(optimizer)\n        scaler.update()\n\n        probs = torch.sigmoid(logits.detach())\n        update_metrics(metrics, probs, y)\n\n        loop.set_postfix(loss=loss.item())\n    return compute_and_reset(metrics)\n\n\n# ---------- 4. Fit routine with early-stopping -------------------------------\ndef fit(model, train_loader, val_loader,\n        epochs=20, patience=3, lr=3e-4, weight_decay=1e-4,\n        save_path=\"best_auc.pt\"):\n    \"\"\" Fit the model \"\"\"\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    model.to(device)\n    loss_fn = nn.BCEWithLogitsLoss()\n\n    opt = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n    scaler = GradScaler()\n\n    best_auc, epochs_no_improve = 0.0, 0\n    for ep in range(1, epochs + 1):\n        train_metrics = train_one_epoch(model, train_loader, opt, scaler,\n                                        loss_fn, device,\n                                        make_metric_dict(device), ep)\n        val_metrics = validate(model, val_loader, loss_fn, device,\n                               make_metric_dict(device))\n\n        print(f\"\\nEpoch {ep}: \"\n              f\"AUROC {val_metrics['auroc']:.4f}  \"\n              f\"PRAUC {val_metrics['prauc']:.4f}  \"\n              f\"Sens {val_metrics['sens']:.4f}  \"\n              f\"Spec {val_metrics['spec']:.4f}\")\n\n        cur_auc = val_metrics[\"auroc\"]\n        if cur_auc > best_auc:\n            best_auc = cur_auc\n            torch.save(model.state_dict(), save_path)\n            epochs_no_improve = 0\n            print(\"  ↑ saved new best weights\")\n        else:\n            epochs_no_improve += 1\n            if epochs_no_improve >= patience:\n                print(f\"Early-stopping (no AUROC improvement for {patience} epochs)\")\n                break\n\n\n# ---------- 5. Example usage --------------------------------------------------\nif __name__ == \"__main__\":\n    # Assume you already have:\n    #   train_idx.parquet  val_idx.parquet  full_metadata.parquet\n    # And a DataLoader class `CQ500DataLoader25D` returning\n    #   x: (9, H, W) float32  |  y: binary label 0/1  (ICH-majority)\n\n    #from data_loader_25d import CQ500DataLoader25D   # ← adjust import\n\n    train_idx = list(set(pd.read_parquet(\"/kaggle/working/train_patients.parquet\")[\"name\"].tolist()))\n    val_idx = list(set(pd.read_parquet(\"/kaggle/working/val_patients.parquet\")[\"name\"].tolist()))\n    \n    train_set = CQ500DataLoader25D(\"/kaggle/working/b1_metadata.parquet\",\n                                   indices=train_idx)\n    val_set   = CQ500DataLoader25D(\"/kaggle/working/b1_metadata.parquet\",\n                                   indices=val_idx)\n    train_set.preprocess(output_dir=\"pp_train_tensors\", chunk_size=100, max_ram_gb=20)\n\n    #current_train_loader = DataLoader(train_set, batch_size=32, shuffle=True, num_workers=4, pin_memory=True)\n    #current_val_loader   = DataLoader(val_set, batch_size=32, shuffle=False, num_workers=4, pin_memory=True)\n\n    #current_model = TwoPointFiveD(backbone_name=\"resnet18\", in_channels=3)\n    #fit(current_model, current_train_loader, current_val_loader, epochs=30, patience=3)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-23T08:30:34.243675Z","iopub.execute_input":"2025-07-23T08:30:34.244377Z","iopub.status.idle":"2025-07-23T08:37:16.227651Z","shell.execute_reply.started":"2025-07-23T08:30:34.244330Z","shell.execute_reply":"2025-07-23T08:37:16.226352Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[{"name":"stdout","text":"[Preprocess] Saving tensors to pp_train_tensors             in chunks of 100 slices, max RAM 20 GB...\n  Saved 100 slices to pp_train_tensors/chunk_00000.pt\n  Saved 100 slices to pp_train_tensors/chunk_00001.pt\n  Saved 100 slices to pp_train_tensors/chunk_00002.pt\n  Saved 100 slices to pp_train_tensors/chunk_00003.pt\n  Saved 100 slices to pp_train_tensors/chunk_00004.pt\n  Saved 100 slices to pp_train_tensors/chunk_00005.pt\n  Saved 100 slices to pp_train_tensors/chunk_00006.pt\n  Saved 100 slices to pp_train_tensors/chunk_00007.pt\n  Saved 100 slices to pp_train_tensors/chunk_00008.pt\n  Saved 100 slices to pp_train_tensors/chunk_00009.pt\n  Saved 100 slices to pp_train_tensors/chunk_00010.pt\n  Saved 100 slices to pp_train_tensors/chunk_00011.pt\n  Saved 100 slices to pp_train_tensors/chunk_00012.pt\n  Saved 100 slices to pp_train_tensors/chunk_00013.pt\n  Saved 100 slices to pp_train_tensors/chunk_00014.pt\n  Saved 100 slices to pp_train_tensors/chunk_00015.pt\n  Saved 100 slices to pp_train_tensors/chunk_00016.pt\n  Saved 100 slices to pp_train_tensors/chunk_00017.pt\n  Saved 100 slices to pp_train_tensors/chunk_00018.pt\n  Saved 100 slices to pp_train_tensors/chunk_00019.pt\n  Saved 100 slices to pp_train_tensors/chunk_00020.pt\n  Saved 100 slices to pp_train_tensors/chunk_00021.pt\n  Saved 100 slices to pp_train_tensors/chunk_00022.pt\n  Saved 100 slices to pp_train_tensors/chunk_00023.pt\n  Saved 100 slices to pp_train_tensors/chunk_00024.pt\n  Saved 100 slices to pp_train_tensors/chunk_00025.pt\n  Saved 100 slices to pp_train_tensors/chunk_00026.pt\n  Saved 100 slices to pp_train_tensors/chunk_00027.pt\n  Saved 100 slices to pp_train_tensors/chunk_00028.pt\n  Saved 100 slices to pp_train_tensors/chunk_00029.pt\n  Saved 100 slices to pp_train_tensors/chunk_00030.pt\n  Saved 100 slices to pp_train_tensors/chunk_00031.pt\n  Saved 100 slices to pp_train_tensors/chunk_00032.pt\n  Saved 100 slices to pp_train_tensors/chunk_00033.pt\n  Saved 100 slices to pp_train_tensors/chunk_00034.pt\n  Saved 100 slices to pp_train_tensors/chunk_00035.pt\n  Saved 100 slices to pp_train_tensors/chunk_00036.pt\n  Saved 100 slices to pp_train_tensors/chunk_00037.pt\n  Saved 100 slices to pp_train_tensors/chunk_00038.pt\n  Saved 100 slices to pp_train_tensors/chunk_00039.pt\n  Saved 100 slices to pp_train_tensors/chunk_00040.pt\n  Saved 100 slices to pp_train_tensors/chunk_00041.pt\n  Saved 100 slices to pp_train_tensors/chunk_00042.pt\n  Saved 100 slices to pp_train_tensors/chunk_00043.pt\n  Saved 100 slices to pp_train_tensors/chunk_00044.pt\n  Saved 100 slices to pp_train_tensors/chunk_00045.pt\n  Saved 100 slices to pp_train_tensors/chunk_00046.pt\n  Saved 100 slices to pp_train_tensors/chunk_00047.pt\n  Saved 100 slices to pp_train_tensors/chunk_00048.pt\n  Saved 100 slices to pp_train_tensors/chunk_00049.pt\n  Saved 100 slices to pp_train_tensors/chunk_00050.pt\n  Saved 100 slices to pp_train_tensors/chunk_00051.pt\n  Saved 100 slices to pp_train_tensors/chunk_00052.pt\n  Saved 100 slices to pp_train_tensors/chunk_00053.pt\n  Saved 100 slices to pp_train_tensors/chunk_00054.pt\n  Saved 100 slices to pp_train_tensors/chunk_00055.pt\n  Saved 100 slices to pp_train_tensors/chunk_00056.pt\n  Saved 100 slices to pp_train_tensors/chunk_00057.pt\n  Saved 100 slices to pp_train_tensors/chunk_00058.pt\n  Saved 100 slices to pp_train_tensors/chunk_00059.pt\n  Saved 100 slices to pp_train_tensors/chunk_00060.pt\n  Saved 100 slices to pp_train_tensors/chunk_00061.pt\n  Saved 100 slices to pp_train_tensors/chunk_00062.pt\n  Saved 100 slices to pp_train_tensors/chunk_00063.pt\n  Saved 100 slices to pp_train_tensors/chunk_00064.pt\n  Saved 100 slices to pp_train_tensors/chunk_00065.pt\n  Saved 100 slices to pp_train_tensors/chunk_00066.pt\n  Saved 100 slices to pp_train_tensors/chunk_00067.pt\n  Saved 100 slices to pp_train_tensors/chunk_00068.pt\n  Saved 100 slices to pp_train_tensors/chunk_00069.pt\n  Saved 100 slices to pp_train_tensors/chunk_00070.pt\n  Saved 100 slices to pp_train_tensors/chunk_00071.pt\n  Saved 100 slices to pp_train_tensors/chunk_00072.pt\n  Saved 100 slices to pp_train_tensors/chunk_00073.pt\n  Saved 100 slices to pp_train_tensors/chunk_00074.pt\n  Saved 100 slices to pp_train_tensors/chunk_00075.pt\n  Saved 100 slices to pp_train_tensors/chunk_00076.pt\n  Saved 100 slices to pp_train_tensors/chunk_00077.pt\n  Saved 100 slices to pp_train_tensors/chunk_00078.pt\n  Saved 100 slices to pp_train_tensors/chunk_00079.pt\n  Saved 100 slices to pp_train_tensors/chunk_00080.pt\n  Saved 100 slices to pp_train_tensors/chunk_00081.pt\n  Saved 100 slices to pp_train_tensors/chunk_00082.pt\n  Saved 100 slices to pp_train_tensors/chunk_00083.pt\n  Saved 100 slices to pp_train_tensors/chunk_00084.pt\n  Saved 100 slices to pp_train_tensors/chunk_00085.pt\n  Saved 100 slices to pp_train_tensors/chunk_00086.pt\n  Saved 100 slices to pp_train_tensors/chunk_00087.pt\n  Saved 100 slices to pp_train_tensors/chunk_00088.pt\n  Saved 100 slices to pp_train_tensors/chunk_00089.pt\n  Saved 100 slices to pp_train_tensors/chunk_00090.pt\n  Saved 100 slices to pp_train_tensors/chunk_00091.pt\n  Saved 100 slices to pp_train_tensors/chunk_00092.pt\n  Saved 100 slices to pp_train_tensors/chunk_00093.pt\n  Saved 100 slices to pp_train_tensors/chunk_00094.pt\n  Saved 100 slices to pp_train_tensors/chunk_00095.pt\n  Saved 100 slices to pp_train_tensors/chunk_00096.pt\n  Saved 100 slices to pp_train_tensors/chunk_00097.pt\n  Saved 100 slices to pp_train_tensors/chunk_00098.pt\n  Saved 100 slices to pp_train_tensors/chunk_00099.pt\n  Saved 100 slices to pp_train_tensors/chunk_00100.pt\n  Saved 100 slices to pp_train_tensors/chunk_00101.pt\n  Saved 100 slices to pp_train_tensors/chunk_00102.pt\n  Saved 100 slices to pp_train_tensors/chunk_00103.pt\n  Saved 100 slices to pp_train_tensors/chunk_00104.pt\n  Saved 100 slices to pp_train_tensors/chunk_00105.pt\n  Saved 100 slices to pp_train_tensors/chunk_00106.pt\n  Saved 100 slices to pp_train_tensors/chunk_00107.pt\n  Saved 100 slices to pp_train_tensors/chunk_00108.pt\n  Saved 100 slices to pp_train_tensors/chunk_00109.pt\n  Saved 100 slices to pp_train_tensors/chunk_00110.pt\n  Saved 100 slices to pp_train_tensors/chunk_00111.pt\n  Saved 100 slices to pp_train_tensors/chunk_00112.pt\n  Saved 100 slices to pp_train_tensors/chunk_00113.pt\n  Saved 100 slices to pp_train_tensors/chunk_00114.pt\n  Saved 100 slices to pp_train_tensors/chunk_00115.pt\n  Saved 100 slices to pp_train_tensors/chunk_00116.pt\n  Saved 100 slices to pp_train_tensors/chunk_00117.pt\n  Saved 100 slices to pp_train_tensors/chunk_00118.pt\n  Saved 100 slices to pp_train_tensors/chunk_00119.pt\n  Saved 100 slices to pp_train_tensors/chunk_00120.pt\n  Saved 100 slices to pp_train_tensors/chunk_00121.pt\n  Saved 100 slices to pp_train_tensors/chunk_00122.pt\n  Saved 100 slices to pp_train_tensors/chunk_00123.pt\n  Saved 100 slices to pp_train_tensors/chunk_00124.pt\n  Saved 100 slices to pp_train_tensors/chunk_00125.pt\n  Saved 100 slices to pp_train_tensors/chunk_00126.pt\n  Saved 100 slices to pp_train_tensors/chunk_00127.pt\n  Saved 100 slices to pp_train_tensors/chunk_00128.pt\n  Saved 100 slices to pp_train_tensors/chunk_00129.pt\n  Saved 100 slices to pp_train_tensors/chunk_00130.pt\n  Saved 100 slices to pp_train_tensors/chunk_00131.pt\n  Saved 100 slices to pp_train_tensors/chunk_00132.pt\n  Saved 100 slices to pp_train_tensors/chunk_00133.pt\n  Saved 100 slices to pp_train_tensors/chunk_00134.pt\n  Saved 100 slices to pp_train_tensors/chunk_00135.pt\n  Saved 100 slices to pp_train_tensors/chunk_00136.pt\n  Saved 100 slices to pp_train_tensors/chunk_00137.pt\n  Saved 100 slices to pp_train_tensors/chunk_00138.pt\n  Saved 100 slices to pp_train_tensors/chunk_00139.pt\n  Saved 100 slices to pp_train_tensors/chunk_00140.pt\n  Saved 100 slices to pp_train_tensors/chunk_00141.pt\n  Saved 100 slices to pp_train_tensors/chunk_00142.pt\n  Saved 100 slices to pp_train_tensors/chunk_00143.pt\n  Saved 100 slices to pp_train_tensors/chunk_00144.pt\n  Saved 100 slices to pp_train_tensors/chunk_00145.pt\n  Saved 100 slices to pp_train_tensors/chunk_00146.pt\n  Saved 100 slices to pp_train_tensors/chunk_00147.pt\n  Saved 100 slices to pp_train_tensors/chunk_00148.pt\n  Saved 100 slices to pp_train_tensors/chunk_00149.pt\n  Saved 100 slices to pp_train_tensors/chunk_00150.pt\n  Saved 100 slices to pp_train_tensors/chunk_00151.pt\n  Saved 100 slices to pp_train_tensors/chunk_00152.pt\n  Saved 100 slices to pp_train_tensors/chunk_00153.pt\n  Saved 100 slices to pp_train_tensors/chunk_00154.pt\n  Saved 100 slices to pp_train_tensors/chunk_00155.pt\n  Saved 100 slices to pp_train_tensors/chunk_00156.pt\n  Saved 100 slices to pp_train_tensors/chunk_00157.pt\n  Saved 100 slices to pp_train_tensors/chunk_00158.pt\n  Saved 100 slices to pp_train_tensors/chunk_00159.pt\n  Saved 100 slices to pp_train_tensors/chunk_00160.pt\n  Saved 100 slices to pp_train_tensors/chunk_00161.pt\n  Saved 100 slices to pp_train_tensors/chunk_00162.pt\n  Saved 100 slices to pp_train_tensors/chunk_00163.pt\n  Saved 100 slices to pp_train_tensors/chunk_00164.pt\n  Saved 100 slices to pp_train_tensors/chunk_00165.pt\n  Saved 100 slices to pp_train_tensors/chunk_00166.pt\n  Saved 100 slices to pp_train_tensors/chunk_00167.pt\n  Saved 100 slices to pp_train_tensors/chunk_00168.pt\n  Saved 100 slices to pp_train_tensors/chunk_00169.pt\n  Saved 100 slices to pp_train_tensors/chunk_00170.pt\n  Saved 100 slices to pp_train_tensors/chunk_00171.pt\n  Saved 100 slices to pp_train_tensors/chunk_00172.pt\n  Saved 100 slices to pp_train_tensors/chunk_00173.pt\n  Saved 100 slices to pp_train_tensors/chunk_00174.pt\n  Saved 100 slices to pp_train_tensors/chunk_00175.pt\n  Saved 100 slices to pp_train_tensors/chunk_00176.pt\n  Saved 100 slices to pp_train_tensors/chunk_00177.pt\n  Saved 100 slices to pp_train_tensors/chunk_00178.pt\n  Saved 100 slices to pp_train_tensors/chunk_00179.pt\n  Saved 100 slices to pp_train_tensors/chunk_00180.pt\n  Saved 100 slices to pp_train_tensors/chunk_00181.pt\n  Saved 100 slices to pp_train_tensors/chunk_00182.pt\n  Saved 100 slices to pp_train_tensors/chunk_00183.pt\n  Saved 100 slices to pp_train_tensors/chunk_00184.pt\n  Saved 100 slices to pp_train_tensors/chunk_00185.pt\n  Saved 100 slices to pp_train_tensors/chunk_00186.pt\n  Saved 100 slices to pp_train_tensors/chunk_00187.pt\n  Saved 100 slices to pp_train_tensors/chunk_00188.pt\n  Saved 100 slices to pp_train_tensors/chunk_00189.pt\n  Saved 100 slices to pp_train_tensors/chunk_00190.pt\n  Saved 100 slices to pp_train_tensors/chunk_00191.pt\n  Saved 100 slices to pp_train_tensors/chunk_00192.pt\n  Saved 100 slices to pp_train_tensors/chunk_00193.pt\n  Saved 100 slices to pp_train_tensors/chunk_00194.pt\n  Saved 100 slices to pp_train_tensors/chunk_00195.pt\n  Saved 100 slices to pp_train_tensors/chunk_00196.pt\n  Saved 100 slices to pp_train_tensors/chunk_00197.pt\n  Saved 100 slices to pp_train_tensors/chunk_00198.pt\n  Saved 100 slices to pp_train_tensors/chunk_00199.pt\n  Saved 100 slices to pp_train_tensors/chunk_00200.pt\n  Saved 100 slices to pp_train_tensors/chunk_00201.pt\n  Saved 100 slices to pp_train_tensors/chunk_00202.pt\n  Saved 100 slices to pp_train_tensors/chunk_00203.pt\n  Saved 100 slices to pp_train_tensors/chunk_00204.pt\n  Saved 100 slices to pp_train_tensors/chunk_00205.pt\n  Saved 100 slices to pp_train_tensors/chunk_00206.pt\n  Saved 100 slices to pp_train_tensors/chunk_00207.pt\n  Saved 100 slices to pp_train_tensors/chunk_00208.pt\n  Saved 100 slices to pp_train_tensors/chunk_00209.pt\n  Saved 100 slices to pp_train_tensors/chunk_00210.pt\n  Saved 100 slices to pp_train_tensors/chunk_00211.pt\n  Saved 100 slices to pp_train_tensors/chunk_00212.pt\n  Saved 100 slices to pp_train_tensors/chunk_00213.pt\n  Saved 100 slices to pp_train_tensors/chunk_00214.pt\n  Saved 100 slices to pp_train_tensors/chunk_00215.pt\n  Saved 100 slices to pp_train_tensors/chunk_00216.pt\n  Saved 100 slices to pp_train_tensors/chunk_00217.pt\n  Saved 100 slices to pp_train_tensors/chunk_00218.pt\n  Saved 100 slices to pp_train_tensors/chunk_00219.pt\n  Saved 100 slices to pp_train_tensors/chunk_00220.pt\n  Saved 100 slices to pp_train_tensors/chunk_00221.pt\n  Saved 100 slices to pp_train_tensors/chunk_00222.pt\n  Saved 100 slices to pp_train_tensors/chunk_00223.pt\n  Saved 100 slices to pp_train_tensors/chunk_00224.pt\n  Saved 100 slices to pp_train_tensors/chunk_00225.pt\n  Saved 100 slices to pp_train_tensors/chunk_00226.pt\n  Saved 100 slices to pp_train_tensors/chunk_00227.pt\n  Saved 100 slices to pp_train_tensors/chunk_00228.pt\n  Saved 100 slices to pp_train_tensors/chunk_00229.pt\n  Saved 100 slices to pp_train_tensors/chunk_00230.pt\n  Saved 100 slices to pp_train_tensors/chunk_00231.pt\n  Saved 100 slices to pp_train_tensors/chunk_00232.pt\n  Saved 100 slices to pp_train_tensors/chunk_00233.pt\n  Saved 100 slices to pp_train_tensors/chunk_00234.pt\n  Saved 100 slices to pp_train_tensors/chunk_00235.pt\n  Saved 100 slices to pp_train_tensors/chunk_00236.pt\n  Saved 100 slices to pp_train_tensors/chunk_00237.pt\n  Saved 100 slices to pp_train_tensors/chunk_00238.pt\n  Saved 100 slices to pp_train_tensors/chunk_00239.pt\n  Saved 100 slices to pp_train_tensors/chunk_00240.pt\n  Saved 100 slices to pp_train_tensors/chunk_00241.pt\n  Saved 100 slices to pp_train_tensors/chunk_00242.pt\n  Saved 100 slices to pp_train_tensors/chunk_00243.pt\n  Saved 100 slices to pp_train_tensors/chunk_00244.pt\n  Saved 100 slices to pp_train_tensors/chunk_00245.pt\n  Saved 100 slices to pp_train_tensors/chunk_00246.pt\n  Saved 100 slices to pp_train_tensors/chunk_00247.pt\n  Saved 100 slices to pp_train_tensors/chunk_00248.pt\n  Saved 100 slices to pp_train_tensors/chunk_00249.pt\n  Saved 100 slices to pp_train_tensors/chunk_00250.pt\n  Saved 100 slices to pp_train_tensors/chunk_00251.pt\n  Saved 100 slices to pp_train_tensors/chunk_00252.pt\n  Saved 100 slices to pp_train_tensors/chunk_00253.pt\n  Saved 100 slices to pp_train_tensors/chunk_00254.pt\n  Saved 100 slices to pp_train_tensors/chunk_00255.pt\n  Saved 100 slices to pp_train_tensors/chunk_00256.pt\n  Saved 100 slices to pp_train_tensors/chunk_00257.pt\n  Saved 100 slices to pp_train_tensors/chunk_00258.pt\n  Saved 100 slices to pp_train_tensors/chunk_00259.pt\n  Saved 100 slices to pp_train_tensors/chunk_00260.pt\n  Saved 100 slices to pp_train_tensors/chunk_00261.pt\n  Saved 100 slices to pp_train_tensors/chunk_00262.pt\n  Saved 100 slices to pp_train_tensors/chunk_00263.pt\n  Saved 100 slices to pp_train_tensors/chunk_00264.pt\n  Saved 100 slices to pp_train_tensors/chunk_00265.pt\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization, _disable_byteorder_record)\u001b[0m\n\u001b[1;32m    943\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0m_open_zipfile_writer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_zipfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 944\u001b[0;31m             _save(\n\u001b[0m\u001b[1;32m    945\u001b[0m                 \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_save\u001b[0;34m(obj, zip_file, pickle_module, pickle_protocol, _disable_byteorder_record)\u001b[0m\n\u001b[1;32m   1215\u001b[0m             \u001b[0;31m# Now that it is on the CPU we can directly copy it into the zip file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1216\u001b[0;31m             \u001b[0mzip_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite_record\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstorage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_bytes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: [enforce fail at inline_container.cc:815] . PytorchStreamWriter failed writing file data/0: file write failed","\nDuring handling of the above exception, another exception occurred:\n","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_110/66361596.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    189\u001b[0m     val_set   = CQ500DataLoader25D(\"/kaggle/working/b1_metadata.parquet\",\n\u001b[1;32m    190\u001b[0m                                    indices=val_idx)\n\u001b[0;32m--> 191\u001b[0;31m     \u001b[0mtrain_set\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"pp_train_tensors\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunk_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_ram_gb\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    192\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    193\u001b[0m     \u001b[0;31m#current_train_loader = DataLoader(train_set, batch_size=32, shuffle=True, num_workers=4, pin_memory=True)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_110/3348375907.py\u001b[0m in \u001b[0;36mpreprocess\u001b[0;34m(self, output_dir, chunk_size, max_ram_gb)\u001b[0m\n\u001b[1;32m    217\u001b[0m                     \u001b[0mchunk_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf\"chunk_{chunk_idx:05d}.pt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m                     \u001b[0mmeta_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf\"chunk_{chunk_idx:05d}_meta.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 219\u001b[0;31m                     \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunk_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    220\u001b[0m                     pd.DataFrame(\n\u001b[1;32m    221\u001b[0m                         \u001b[0mmeta_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"patient_name\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"instance_num\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"dcm_path\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization, _disable_byteorder_record)\u001b[0m\n\u001b[1;32m    941\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    942\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_use_new_zipfile_serialization\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 943\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0m_open_zipfile_writer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_zipfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    944\u001b[0m             _save(\n\u001b[1;32m    945\u001b[0m                 \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    782\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    783\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 784\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfile_like\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite_end_of_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    785\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfile_stream\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    786\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfile_stream\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: [enforce fail at inline_container.cc:626] . unexpected pos 448 vs 342"],"ename":"RuntimeError","evalue":"[enforce fail at inline_container.cc:626] . unexpected pos 448 vs 342","output_type":"error"}],"execution_count":23}]}